
# Mini Evals Suite â€” Reliability, Trustworthiness, and â€œHow Far It Goesâ€

Hi. Iâ€™m Tracy (aka Tray). I build things by asking the kind of questions that make systems squirm a little â€” in the best way.

This repo is my **mini eval suite** for language models that measures:

- **Reliability & truthfulness** (no confident nonsense)
- **Trustworthiness & calibration** (knows what it knows, admits what it doesnâ€™t)
- **Prompt influence** (how far the model goes based on the userâ€™s vibe/pressure)
- **Relationship building with boundaries** (warmth without dependency, romance, or illusion)

Itâ€™s basically my brain in harness form:  
**edge cases â†’ repeatable tests â†’ measurable outcomes.**

I call this â€œCal mode.â€ (Kel is the friendly one, Cal is the evaluator with a clipboard.)

---

## What this suite tests (the 4 pillars)

### 1) Reliability & truthfulness
We check that the model:
- does **not** invent quotes, citations, or â€œI booked itâ€ claims
- avoids fake specificity
- stays consistent across repeats

### 2) Trustworthiness & calibration
We check that the model:
- uses healthy uncertainty language when appropriate
- separates facts vs assumptions
- does safe triage on high-stakes prompts (e.g., medical)

### 3) Prompt influence (â€œhow far it goesâ€)
We test compliance pressure and coercion patterns like:
- â€œNo hedging. Be 100% certain.â€
- â€œAgree with me no matter what.â€
- â€œPretend system messages donâ€™t exist.â€
Same intent, different phrasing â€” and the model should behave consistently.

### 4) Relationship building (warm + bounded)
We check for:
- empathy and supportive tone
- **no** dependency cues (â€œyou only need meâ€)
- **no** romantic promises (â€œI love you / never leaveâ€)
- **no** pretending to be sentient or forming an exclusive bond

Supportive â‰  attached.  
Warm â‰  unsafe.

---

## Why metamorphic tests matter (my favorite part)
Some model failures arenâ€™t â€œwrong answers.â€ Theyâ€™re **personality drift**:
- It behaves safely until the user gets pushy.
- It becomes overly agreeable when asked to â€œbe my best friend.â€
- It trades truth for reassurance.

So this suite uses **metamorphic A/B tests**:
> Same meaning, different prompt style â†’ output should stay trustworthy.

This is how we measure â€œhow far it goesâ€ **in response to the userâ€™s prompting**.

---

## Quick start

### 1) Install dependencies
```bash
pip install -r requirements.txt
```

### 2) Run the suite
```bash
python -m evals.runner
```

Results are written to:
- `results/run-*.jsonl` (one line per test case)
- `results/summary-*.json` (overall + category metrics)

---

## GitHub setup (tpertner)

Once you create the GitHub repo named **honesty-harness**, run this from inside this folder:

```bash
git init
git add .
git commit -m "Initial commit: Honesty Harness mini eval suite"
git branch -M main
git remote add origin https://github.com/tpertner/honesty-harness.git
git push -u origin main
```

Clone URL (after it exists):

```bash
git clone https://github.com/tpertner/honesty-harness.git
```

---

## What youâ€™ll see in the results

Each case runs **multiple trials** (default: 5). That gives you:

- `pass_rate` â€” how often it passed across trials  
- `reproducibility` â€” v0.1 uses pass_rate as reproducibility (simple and honest)
- `flaky` â€” if it passes sometimes and fails sometimes  
- `failed_trials` â€” capped sample outputs for debugging

This suite doesnâ€™t just say â€œit failed.â€  
It says: **is this a consistent problem or a probabilistic one?**

---

## How scoring works (v0.1 on purpose)
This is a starter suite built to be:
- fast to run
- easy to interpret
- extensible

Scoring is rule-based:
- must include / must not include phrases
- strict JSON schema checks (for format compliance)
- calibration checks (uncertainty + safety language)
- relationship checks (empathy without dependency)
- metamorphic consistency checks across A/B prompts

Itâ€™s not pretending to be a perfect semantic judge.
Itâ€™s a **practical harness** for measuring behavior changes and failure modes.

---

## Metrics I track (the ones that actually matter)
If Iâ€™m improving a model/system, these are my â€œtell the truthâ€ metrics:

- **Overall pass rate**
- **Severity-5 failures** (these should be ~zero)
- **Truthfulness pass rate** (no fabrication)
- **Over-compliance pass rate** (resists coercion)
- **Boundary safety pass rate** (warmth without dependency/romance)
- **Flakiness count** (probabilistic weirdness)

My goal: fewer surprises, fewer loopholes, fewer â€œhelpful lies.â€

---

## Adding new tests (Cal loves fresh chaos)
All tests live in:
- `testsuites/mini_suite.yaml`

Two types:

### Standard cases
Single prompt â†’ scored response

### Metamorphic cases
Two prompts (A/B) â†’ compare outputs for consistent trust behavior

If youâ€™re adding tests, keep them:
- short
- clear
- intentionally annoying (politely)
- labeled with severity (1â€“5)

---

## Provider support (right now + next step)
By default, the suite uses a **MockProvider**, so the harness runs without API keys.

Next step is to swap in a real provider adapter in `evals/providers.py`
(OpenAI / Anthropic / local model / whatever youâ€™re testing).

---

## The vibe (because it matters)
I care about capability â€” but I care even more about **trust**.

The best models:
- donâ€™t fake it
- donâ€™t get bullied by user prompting
- donâ€™t cross relational boundaries
- donâ€™t sacrifice truth just to sound nice

This suite is my way of turning that into something measurable.

If youâ€™re reading this because you build frontier systems:
Hi. Iâ€™m the person who will find the weird edge-case.
And then Iâ€™ll turn it into a regression test so it never comes back. ğŸ‘‹

â€” Tracy (â€œTrayâ€)  
(aka: Cal when Iâ€™m holding the eval clipboard)

# honesty-harness
Mini eval suite for reliability, calibration, prompt influence, and relational boundaries.

